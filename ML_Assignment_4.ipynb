{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b25ed7-4471-4210-a866-9413132ce988",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8c7fe-2547-4f5a-b577-2a28719ac30b",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of data in one or more fields for one or more observations. In other words, some data points or features in a dataset have not been collected or are unknown. Missing values can occur due to various reasons, such as faulty data collection methods, human errors, or system failures.\n",
    "\n",
    "It is essential to handle missing values in a dataset because they can affect the accuracy and reliability of the results obtained from the analysis. If missing values are not handled correctly, they can lead to biased or incomplete results, which can negatively impact decision-making based on the analysis.\n",
    "\n",
    "There are some algorithms which are not affected by missing value - \n",
    "\n",
    "    1. Decision Trees: Decision trees can handle missing values by excluding them from the splitting criteria or by treating them as a separate branch.\n",
    "\n",
    "    2. Random Forests: Random forests can handle missing values in a similar way to decision trees by excluding them from the splitting criteria.\n",
    "\n",
    "    3. K-Nearest Neighbors (KNN): KNN can handle missing values by imputing the missing values with the mean, median, or mode of the nearest neighbors.\n",
    "\n",
    "    4. Naive Bayes: Naive Bayes can handle missing values by ignoring the missing values during the calculation of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb077dd-000f-486d-85cb-8efc28870449",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8e47b-906e-4951-8c2a-dc03060ca91f",
   "metadata": {},
   "source": [
    "There are several techniques used to handle missing data in a dataset. Some commonly used techniques are:\n",
    "\n",
    "    1. Deletion: Deletion involves removing observations or features with missing data. This technique is simple and straightforward but may result in a loss of valuable information.\n",
    "    \n",
    "    2. Imputation: Imputation involves replacing missing values with estimated values. This technique can be useful for preserving valuable information, but the estimated values may introduce bias into the analysis.\n",
    "    \n",
    "    3. Prediction: Prediction involves using machine learning algorithms to predict missing values based on the observed data. This technique can be useful for preserving valuable information and minimizing bias, but it can be computationally expensive and may require a large dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21c5b0cb-b729-4892-90b3-35e66369b5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== Example of deletion method ======================\n",
      "   Name   Age   Salary\n",
      "0  John  25.0  50000.0\n",
      "1  Jane  27.0  60000.0\n",
      "3  Mary  30.0  70000.0\n",
      "=================== Example of imputation method ======================\n",
      "   Name        Age   Salary\n",
      "0  John  25.000000  50000.0\n",
      "1  Jane  27.000000  60000.0\n",
      "2  Mike  27.333333  60000.0\n",
      "3  Mary  30.000000  70000.0\n",
      "=================== Example of prediction method ======================\n",
      "   Name        Age   Salary\n",
      "0  John  25.000000  50000.0\n",
      "1  Jane  27.000000  60000.0\n",
      "3  Mary  30.000000  70000.0\n",
      "2  Mike  27.333333  60000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223/3199584470.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['Salary'] = model.predict(df_test[['Age']])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "data = {'Name': ['John', 'Jane', 'Mike', 'Mary'],\n",
    "        'Age': [25, 27, None, 30],\n",
    "        'Salary': [50000, 60000, None, 70000]}\n",
    "df = pd.DataFrame(data)\n",
    "df_imputed = pd.DataFrame(data)\n",
    "\n",
    "print(\"=================== Example of deletion method ======================\")\n",
    "# drop rows with missing values\n",
    "df_dropna = df.dropna()\n",
    "print(df_dropna)\n",
    "\n",
    "print(\"=================== Example of imputation method ======================\")\n",
    "# impute missing values with mean values\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "# print(df_imputed)\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "df_imputed.Age = imputer.fit_transform(df_imputed['Age'].values.reshape(-1,1))[:,0]\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy='mean') \n",
    "df_imputed.Salary = imputer.fit_transform(df_imputed['Salary'].values.reshape(-1,1))[:,0]\n",
    "print(df_imputed)\n",
    "\n",
    "print(\"=================== Example of prediction method ======================\")\n",
    "# split the dataset into training and test sets\n",
    "df.Age = imputer.fit_transform(df['Age'].values.reshape(-1,1))[:,0]\n",
    "df_train = df.dropna()\n",
    "df_test = df[df.isna().any(axis=1)]\n",
    "#print(df_train)\n",
    "#print(df_test)\n",
    "\n",
    "\n",
    "# train a linear regression model on the training set\n",
    "model = LinearRegression()\n",
    "model.fit(df_train[['Age']], df_train['Salary'])\n",
    "\n",
    "# predict missing values in the test set\n",
    "df_test['Salary'] = model.predict(df_test[['Age']])\n",
    "df_imputed = pd.concat([df_train, df_test], axis=0)\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec790bc3-ce6e-4f2d-9291-8b4b8642f745",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa3310-fd67-471e-8dcf-1bfb985cb56a",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of classes in a dataset is unequal, meaning that the number of instances in one class greatly exceeds the number of instances in the other class(es). For example, in a binary classification problem where one class represents a rare event (e.g., fraud detection), the majority class may account for 99% of the instances, leaving only 1% of instances in the minority class. This is an example of imbalanced data.\n",
    "\n",
    "When imbalanced data is not handled, it can lead to several issues:\n",
    "\n",
    "    1. Biased model performance: Since the machine learning algorithms are designed to maximize accuracy, they tend to perform poorly on imbalanced datasets. In such a situation, the model often ends up predicting the majority class and ignoring the minority class completely. As a result, the model may have high accuracy but very low recall, which means that it can correctly identify the majority class but misses the minority class. This is not desirable, especially when the minority class is critical and needs to be identified.\n",
    "\n",
    "    2. Overfitting: Imbalanced data can also lead to overfitting, where the model becomes too complex and performs well on the training data but poorly on the test data. This is because the model tries to memorize the minority class and becomes too specialized, ignoring the majority class.\n",
    "\n",
    "    3. Poor generalization: Another issue with imbalanced data is that the model may not generalize well to new, unseen data. Since the model is biased towards the majority class, it may not be able to identify the minority class in new data, which can result in poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce40608-d835-4946-91e8-128857820380",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0064f-7151-448d-b2a6-6d3e7a9db092",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are two common techniques used to handle imbalanced data, where the distribution of classes in a dataset is unequal.\n",
    "\n",
    "Up-sampling involves randomly duplicating instances from the minority class to create a more balanced dataset, while down-sampling involves randomly removing instances from the majority class to create a more balanced dataset.\n",
    "\n",
    "Example - \n",
    "\n",
    "Suppose we have a dataset with 10,000 transactions, of which only 100 are fraudulent. In this case, we have an imbalanced dataset because the majority class (non-fraudulent transactions) accounts for 99% of the instances, while the minority class (fraudulent transactions) accounts for only 1% of the instances. We want to do fraud detection. For that we will increase data for minority class. This is called Up-sampling.\n",
    "\n",
    "If the situation is opposite like 99% transaction is fraudlent transaction and remaining 1% transaction is non-fraudlent transaction, then also dataset will be imbalanced. In this case we will do down-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49d412-d19d-430f-9226-d40e08c9e27a",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70df5c-41f0-4615-ae51-c38b6909497b",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to increase the size and diversity of a dataset by creating new artificial data samples from the existing ones. The goal of data augmentation is to improve the performance of machine learning models by increasing the robustness and generalization ability of the model.\n",
    "\n",
    "One common data augmentation technique is SMOTE (Synthetic Minority Over-sampling Technique), which is used to handle imbalanced data. SMOTE generates new synthetic examples by interpolating between existing examples from the minority class.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "    1. For each example in the minority class, find its k nearest neighbors (k is a user-defined parameter).\n",
    "\n",
    "    2. Select one of the k nearest neighbors randomly and create a new example by interpolating between the original example and the selected neighbor. The interpolation is done by randomly selecting values for each feature from the range defined by the original example and the selected neighbor.\n",
    "    \n",
    "    3.Repeat steps 1 and 2 until the desired number of synthetic examples is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02731da-aa54-48c5-aa63-45bd46b5f492",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f07f0-8c9c-410d-8237-866b2c7e3767",
   "metadata": {},
   "source": [
    "Outliers are data points that are significantly different from other data points in a dataset. Outliers can be caused by errors in data collection, measurement errors, or they may represent extreme values that occur naturally in the data.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the results of statistical analyses and machine learning models. Outliers can skew the results of statistical analyses, making them less reliable and less representative of the overall population. In machine learning, outliers can result in models that are less accurate and less generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f21f42-f5e0-482b-af39-2f89b44df84c",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041532c6-0d8d-40aa-b602-22e95c8c9b82",
   "metadata": {},
   "source": [
    "There are several techniques used to handle missing data in a dataset. Some commonly used techniques are:\n",
    "\n",
    "1. Deletion: Deletion involves removing observations or features with missing data. This technique is simple and straightforward but may result in a loss of valuable information.\n",
    "\n",
    "2. Imputation: Imputation involves replacing missing values with estimated values. This technique can be useful for preserving valuable information, but the estimated values may introduce bias into the analysis.\n",
    "\n",
    "3. Prediction: Prediction involves using machine learning algorithms to predict missing values based on the observed data. This technique can be useful for preserving valuable information and minimizing bias, but it can be computationally expensive and may require a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4de31d-b378-4d16-b56f-46dc53dd6375",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076bcef1-ab1d-4cab-97c8-d7c3374b5596",
   "metadata": {},
   "source": [
    "There are several strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "    Visual inspection: One approach is to visualize the missing data using a heatmap or a scatter plot matrix. This can help identify patterns or correlations between missing values and other variables in the dataset.\n",
    "\n",
    "    Statistical tests: Statistical tests can be used to test if the missing data is missing at random or not. For example, the Little’s MCAR test can be used to test if the data is missing completely at random (MCAR). If the data is not MCAR, then additional tests can be used to determine if the data is missing at random (MAR) or not missing at random (MNAR).\n",
    "\n",
    "    Imputation: Another approach is to use imputation methods to estimate missing values and then compare the imputed data to the observed data. If the imputed data is similar to the observed data, then it may be assumed that the missing data is missing at random.\n",
    "\n",
    "    Domain knowledge: Sometimes domain knowledge can be used to determine if the missing data is missing at random or not. For example, if missing data is more common among older individuals in a health study, it may be assumed that the missing data is not missing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8723a-085a-41d0-af55-dcb46c95578f",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c094e3-f395-48d3-975f-f2dfebf68a8b",
   "metadata": {},
   "source": [
    "Here are some strategies that can be used:\n",
    "\n",
    "    Confusion matrix: A confusion matrix can be used to evaluate the performance of the machine learning model. This matrix provides information on the true positives, false positives, true negatives, and false negatives of the model.\n",
    "\n",
    "    Accuracy measures: While accuracy is a commonly used metric to evaluate model performance, it may not be the best metric in the case of an imbalanced dataset. Other metrics such as precision, recall, and F1 score may provide a better evaluation of the model's performance.\n",
    "\n",
    "    Resampling methods: Resampling methods such as over-sampling and under-sampling can be used to balance the dataset. This can help ensure that the model is trained on an equal number of positive and negative examples, which can improve the performance of the model.\n",
    "\n",
    "    Ensemble models: Ensemble models such as Random Forest or XGBoost can be used to improve the performance of the model on imbalanced datasets. These models can combine multiple weak learners to create a stronger model that is better suited for imbalanced datasets.\n",
    "\n",
    "    Cost-sensitive learning: Cost-sensitive learning involves adjusting the misclassification costs to give more weight to the minority class. This can improve the performance of the model on the minority class while maintaining a reasonable performance on the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dffb787-d358-410b-85ef-ff96006eacad",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349613de-d2ee-4243-bb26-1aa7453bdbf5",
   "metadata": {},
   "source": [
    "Here are some methods that can be employed to down-sample the majority class:\n",
    "\n",
    "    Random under-sampling: This method involves randomly removing some of the samples from the majority class to balance the dataset.\n",
    "    \n",
    "    Cluster-based under-sampling: This method involves clustering the majority class samples and keeping a representative sample from each cluster.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c307bff-2979-4364-8b43-3faf8eb55243",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dde408-0e9f-4f36-ac59-73f350afe28f",
   "metadata": {},
   "source": [
    "Here are some methods that can be employed to up-sample the minority class:\n",
    "\n",
    "    Random over-sampling: This method involves randomly duplicating some of the samples from the minority class to balance the dataset.\n",
    "\n",
    "    Synthetic minority over-sampling technique (SMOTE): This method involves creating synthetic samples for the minority class by interpolating between existing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1e39b-3813-4e33-abfb-9dc4f6ed3255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
